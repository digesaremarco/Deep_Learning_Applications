{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ae2782-57ea-48f0-b948-b412d0076ffc",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning Laboratory\n",
    "\n",
    "In this laboratory session we will work on getting more advanced versions of Deep Reinforcement Learning algorithms up and running. Deep Reinforcement Learning is **hard**, and getting agents to stably train can be frustrating and requires quite a bit of subtlety in analysis of intermediate results. We will start by refactoring (a bit) my implementation of `REINFORCE` on the [Cartpole environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fadaf0c-f9b2-4680-8456-0eadb5eb8c2f",
   "metadata": {},
   "source": [
    "## Exercise 1: Improving my `REINFORCE` Implementation (warm up)\n",
    "\n",
    "In this exercise we will refactor a bit and improve some aspects of my `REINFORCE` implementation. \n",
    "\n",
    "**First Things First**: Spend some time playing with the environment to make sure you understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "id": "241e9bdc-7aa9-4a12-a57f-088fdc87eab6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T10:22:44.536915Z",
     "start_time": "2025-03-31T10:22:41.456522Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import pygame\n",
    "\n",
    "# Instantiate a rendering and a non-rendering environment.\n",
    "env_render = gym.make('CartPole-v1', render_mode='human')\n",
    "env = gym.make('CartPole-v1')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T10:22:58.005872Z",
     "start_time": "2025-03-31T10:22:54.260113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "\n",
    "# Inizializza Weights & Biases\n",
    "wandb.init(project=\"reinforce-cartpole\", name=\"policy_gradient_run\")"
   ],
   "id": "a8640d7683a1d4a9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: marco-digesare (marco-digesare-university-of-florence) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Universit√†\\Deep Learning Applications\\wandb\\run-20250331_122257-fzx717x4</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marco-digesare-university-of-florence/reinforce-cartpole/runs/fzx717x4' target=\"_blank\">policy_gradient_run</a></strong> to <a href='https://wandb.ai/marco-digesare-university-of-florence/reinforce-cartpole' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/marco-digesare-university-of-florence/reinforce-cartpole' target=\"_blank\">https://wandb.ai/marco-digesare-university-of-florence/reinforce-cartpole</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/marco-digesare-university-of-florence/reinforce-cartpole/runs/fzx717x4' target=\"_blank\">https://wandb.ai/marco-digesare-university-of-florence/reinforce-cartpole/runs/fzx717x4</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/marco-digesare-university-of-florence/reinforce-cartpole/runs/fzx717x4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x251d91bbcd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T09:51:11.010717Z",
     "start_time": "2025-03-31T09:51:11.003413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "(obs, info) = env.reset()\n",
    "env.action_space # return the action space of the environment"
   ],
   "id": "c775d1e7b0a51716",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T09:51:11.506436Z",
     "start_time": "2025-03-31T09:51:11.499877Z"
    }
   },
   "cell_type": "code",
   "source": "env.observation_space # return the observation space of the environment",
   "id": "30a176e18810ea81",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T09:51:11.972850Z",
     "start_time": "2025-03-31T09:51:11.966756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# at first we reset to the middle the pendulum position and add noise (it makes the pendulum swing a bit)\n",
    "obs # print the observation"
   ],
   "id": "be663aae8d3e8e8b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04441478, -0.04412647,  0.04048895, -0.03495523], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T09:51:12.611146Z",
     "start_time": "2025-03-31T09:51:12.605563Z"
    }
   },
   "cell_type": "code",
   "source": "env.spec.pprint() # print the environment specification",
   "id": "16b294711da04412",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=CartPole-v1\n",
      "reward_threshold=475.0\n",
      "max_episode_steps=500\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T09:51:13.975429Z",
     "start_time": "2025-03-31T09:51:13.969370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# policy network\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], 128)\n",
    "        self.fc3 = nn.Linear(128, env.action_space.n)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.fc3(x), dim=-1)"
   ],
   "id": "f41c168f5f75b437",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T09:45:21.617022Z",
     "start_time": "2025-03-31T09:45:21.606883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# reinforce algorithm\n",
    "class Reinforce:\n",
    "    def __init__(self, policy, env, gamma=0.99, num_episodes=10):\n",
    "        self.policy = policy\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.num_episodes = num_episodes\n",
    "        self.opt = optim.Adam(policy.parameters(), lr=1e-2) # Adam optimizer\n",
    "\n",
    "    def run_episode(self):\n",
    "        obs, info = self.env.reset()\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        actions = []\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            action_probs = self.policy(torch.tensor(obs, dtype=torch.float32))\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "            log_prob = torch.log(action_probs[action])\n",
    "\n",
    "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            actions.append(action)\n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "        return rewards, log_probs, actions"
   ],
   "id": "52fe42bade9b851c",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T09:46:30.715484Z",
     "start_time": "2025-03-31T09:46:30.666772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example of running an episode\n",
    "policy = PolicyNet(env)\n",
    "reinforce = Reinforce(policy, env)\n",
    "for i in range(10):\n",
    "    # run an episode\n",
    "    rewards, log_probs, actions = reinforce.run_episode()\n",
    "    print(f\"Episode {i+1}: Total Reward: {sum(rewards)}\")\n"
   ],
   "id": "56130c0b361b351f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward: 11.0\n",
      "Episode 2: Total Reward: 34.0\n",
      "Episode 3: Total Reward: 16.0\n",
      "Episode 4: Total Reward: 31.0\n",
      "Episode 5: Total Reward: 14.0\n",
      "Episode 6: Total Reward: 15.0\n",
      "Episode 7: Total Reward: 10.0\n",
      "Episode 8: Total Reward: 22.0\n",
      "Episode 9: Total Reward: 29.0\n",
      "Episode 10: Total Reward: 15.0\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T10:23:16.968535Z",
     "start_time": "2025-03-31T10:23:16.952629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Given an environment, observation, and policy, sample from pi(a | obs). Returns the\n",
    "# selected action and the log probability of that action (needed for policy gradient).\n",
    "def select_action(env, obs, policy): # obs and policy are torch tensors\n",
    "    dist = Categorical(policy(obs)) # sample from the policy\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "    return (action.item(), log_prob.reshape(1)) # return the selected action and the log_prob of that action\n",
    "\n",
    "# Utility to compute the discounted total reward. Torch doesn't like flipped arrays, so we need to\n",
    "# .copy() the final numpy array. There's probably a better way to do this.\n",
    "def compute_returns(rewards, gamma):\n",
    "    return np.flip(np.cumsum([gamma**(i+1)*r for (i, r) in enumerate(rewards)][::-1]), 0).copy() # from an input list of rewards and a discount factor gamma, compute the discounted returns\n",
    "\n",
    "# Given an environment and a policy, run it up to the maximum number of steps.\n",
    "def run_episode(env, policy, maxlen=500): # maxlen is the maximum number of steps of the episode\n",
    "    # Collect just about everything.\n",
    "    observations = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "\n",
    "    # Reset the environment and start the episode.\n",
    "    (obs, info) = env.reset()\n",
    "    for i in range(maxlen):\n",
    "        # Get the current observation, run the policy and select an action.\n",
    "        obs = torch.tensor(obs) # observe the environment and convert the observation to a tensor\n",
    "        (action, log_prob) = select_action(env, obs, policy) # select an action from the policy\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        # Advance the episode by executing the selected action.\n",
    "        (obs, reward, term, trunc, info) = env.step(action) # go to the next step in the environment, term is True if the episode is over, trunc is True if the episode was truncated\n",
    "        rewards.append(reward)\n",
    "        if term or trunc:\n",
    "            break\n",
    "    return (observations, actions, torch.cat(log_probs), rewards) # return the observations, actions, log_probs and rewards of the episode"
   ],
   "id": "487e8f1b263a437e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T10:23:19.335411Z",
     "start_time": "2025-03-31T10:23:19.323369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# A simple, but generic, policy network with one hidden layer.\n",
    "class PolicyNet(nn.Module): # use a deep neural network to represent the policy\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], 128)\n",
    "        self.fc2 = nn.Linear(128, env.action_space.n)\n",
    "\n",
    "    def forward(self, s):\n",
    "        s = F.relu(self.fc1(s))\n",
    "        s = F.softmax(self.fc2(s), dim=-1)\n",
    "        return s"
   ],
   "id": "a781dca7baa85f9b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T10:23:29.101665Z",
     "start_time": "2025-03-31T10:23:29.092076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# A direct, inefficient, and probably buggy of the REINFORCE policy gradient algorithm.\n",
    "def reinforce(policy, env, env_render=None, gamma=0.99, num_episodes=10):\n",
    "    # The only non-vanilla part: we use Adam instead of SGD.\n",
    "    opt = torch.optim.Adam(policy.parameters(), lr=1e-2)\n",
    "\n",
    "    # Track episode rewards in a list.\n",
    "    running_rewards = [0.0] # collect the running rewards of the episodes\n",
    "\n",
    "    # The main training loop.\n",
    "    policy.train()\n",
    "    for episode in range(num_episodes):\n",
    "        # Run an episode of the environment, collect everything needed for policy update.\n",
    "        (observations, actions, log_probs, rewards) = run_episode(env, policy)\n",
    "\n",
    "        # Compute the discounted reward for every step of the episode.\n",
    "        returns = torch.tensor(compute_returns(rewards, gamma), dtype=torch.float32)\n",
    "\n",
    "        # Keep a running average of total discounted rewards for the whole episode.\n",
    "        running_rewards.append(0.05 * returns[0].item() + 0.95 * running_rewards[-1])\n",
    "\n",
    "        # Standardize returns.\n",
    "        returns = (returns - returns.mean()) / returns.std() # standardize the returns, stabilizing the training\n",
    "\n",
    "        # Make an optimization step\n",
    "        opt.zero_grad()\n",
    "        loss = (-log_probs * returns).mean() # mean instead of sum to make the learning rate independent of the number of steps in the episode\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Render an episode after every 100 policy updates.\n",
    "        if not episode % 100:\n",
    "            if env_render:\n",
    "                policy.eval()\n",
    "                run_episode(env_render, policy)\n",
    "                policy.train()\n",
    "            print(f'Running reward: {running_rewards[-1]}')\n",
    "\n",
    "    # Return the running rewards.\n",
    "    policy.eval()\n",
    "    return running_rewards"
   ],
   "id": "8516cdc2502b7b1e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T10:19:44.093732Z",
     "start_time": "2025-03-31T10:19:44.086552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reinforce with W&B\n",
    "def reinforce(policy, env, env_render=None, gamma=0.99, num_episodes=10):\n",
    "    opt = torch.optim.Adam(policy.parameters(), lr=1e-2)\n",
    "    running_rewards = [0.0]\n",
    "\n",
    "    policy.train()\n",
    "    for episode in range(num_episodes):\n",
    "        observations, actions, log_probs, rewards = run_episode(env, policy)\n",
    "        returns = torch.tensor(compute_returns(rewards, gamma), dtype=torch.float32)\n",
    "\n",
    "        running_rewards.append(0.05 * returns[0].item() + 0.95 * running_rewards[-1])\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss = (-log_probs * returns).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Log su wandb\n",
    "        wandb.log({\n",
    "            \"episode\": episode,\n",
    "            \"loss\": loss.item(),\n",
    "            \"reward\": returns[0].item(),\n",
    "            \"running_reward\": running_rewards[-1]\n",
    "        })\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            if env_render:\n",
    "                policy.eval()\n",
    "                run_episode(env_render, policy)\n",
    "                policy.train()\n",
    "            print(f'Running reward: {running_rewards[-1]}')\n",
    "\n",
    "    policy.eval()\n",
    "    return running_rewards"
   ],
   "id": "2241a7bc0394a890",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T10:23:51.118819Z",
     "start_time": "2025-03-31T10:23:46.891981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instantiate a (rendering) CartPole environment.\n",
    "env_render = gym.make('CartPole-v1', render_mode='human')\n",
    "pygame.display.init()  # Might help PyGame not crash...\n",
    "\n",
    "# Make a policy network and run a few episodes to see how well random initialization works.\n",
    "policy = PolicyNet(env_render)\n",
    "for _ in range(10):\n",
    "    run_episode(env_render, policy)\n",
    "\n",
    "# If you don't close the environment, the PyGame window stays visible.\n",
    "env_render.close()\n",
    "\n",
    "# Again we pray PyGame doesn't crash...\n",
    "pygame.display.quit()"
   ],
   "id": "b5403a47484e6164",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T10:25:13.509894Z",
     "start_time": "2025-03-31T10:23:51.841016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# In the new version of Gymnasium you need different environments for rendering and no rendering.\n",
    "# Here we instantiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "# down), and another that does not animate.\n",
    "seed = 2112\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset(seed=seed)\n",
    "env_render = None # gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "# PyGame, please don't crash.\n",
    "pygame.display.init()\n",
    "\n",
    "# Make a policy network.\n",
    "policy = PolicyNet(env)\n",
    "\n",
    "# Train the agent.\n",
    "plt.plot(reinforce(policy, env, env_render, num_episodes=500))\n",
    "\n",
    "# Close up everything\n",
    "#env_render.close()\n",
    "env.close()\n",
    "pygame.display.quit()  # Fingers crossed..."
   ],
   "id": "3151ea7323d644f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward: 0.7352840423583985\n",
      "Running reward: 49.96331935858296\n",
      "Running reward: 62.199887457454466\n",
      "Running reward: 83.76816105347082\n",
      "Running reward: 96.37891060354198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# log of episode_return and episode_length (optional avg_val_length) to make weights and biases plots. episode_return is the total reward of the episode, episode_length is the number of steps in the episode, avg_val_length is the average length of the episodes. Also loss is logged, it's a good metric to see if the agent is learning or not.\n",
    "# at the beginning the loss is low because the agent is not learning, then it increases because the agent is learning. The loss is not a good metric to see if the agent is learning or not, but it's a good metric to see if the agent is overfitting or not. The loss should be low and stable, if it increases too much the agent is overfitting. If it decreases too much the agent is underfitting. The loss should be low and stable, if it increases too much the agent is overfitting. If it decreases too much the agent is underfitting.\n",
    "\n",
    "# vectorized environment is used to parallelize the training of the agent. It is not used in this implementation, but it is a good practice to use it when training agents in reinforcement learning.\n",
    "# No baseline Vs standardized baseline Vs value function baseline\n",
    "\n",
    "# I need a second network to estimate the value function with input = state s and output = v(s). 4 elements in the input vector, 1 element in the output vector. The second network has the same architecture of the policy network. (the last layer changes from 2 to 1)\n",
    "# The output of the second network is used as a baseline in the REINFORCE algorithm.\n",
    "# Gt is a sample of the return of the episode, the second network has to be trained to learn (V(s) - Gt)^alpha"
   ],
   "id": "b49d657fe87b3ad4"
  },
  {
   "cell_type": "code",
   "id": "cbd2dd9d-a60b-4892-878f-d83f64ac5f63",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-03-31T09:27:09.943092Z",
     "start_time": "2025-03-31T09:27:09.937357Z"
    }
   },
   "source": [
    "# Your code here. Call .reset() on the environment and inspect the output. Also, check out the observation_space."
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "0ae31dce-be71-4a86-95fd-1af902f026b8",
   "metadata": {},
   "source": [
    "**Next Things Next**: Now get your `REINFORCE` implementation working on the environment. You can import my (probably buggy and definitely inefficient) implementation here. Or even better, refactor an implementation into a separate package from which you can `import` the stuff you need here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70490822-2859-4787-bf00-3084ad48252a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Your code here. You should be able to train an agent to solve Cartpole. This will be our starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b8a90f-78ed-4f43-9d66-aeba7c74fe2c",
   "metadata": {},
   "source": [
    "**Last Things Last**: My implementation does a **super crappy** job of evaluating the agent performance during training. The running average is not a very good metric. Modify my implementation so that every $N$ iterations (make $N$ an argument to the training function) the agent is run for $M$ episodes in the environment. Collect and return: (1) The average **total** reward received over the $M$ iterations; and (2) the average episode length. Analyze the performance of your agents with these new metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c722e50-f882-430f-903e-8a97d38bd0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5adad7-759b-4000-925b-701f41fe6e97",
   "metadata": {},
   "source": [
    "-----\n",
    "## Exercise 2: `REINFORCE` with a Value Baseline (warm up)\n",
    "\n",
    "In this exercise we will augment my implementation (or your own) of `REINFORCE` to subtract a baseline from the target in the update equation in order to stabilize (and hopefully speed-up) convergence. For now we will stick to the Cartpole environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8b8cf76-5d39-45e9-8d7d-324125c04b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cdd5bc3-a439-4d84-891b-f4a840331e07",
   "metadata": {},
   "source": [
    "**First Things First**: Recall from the slides on Deep Reinforcement Learning that we can **subtract** any function that doesn't depend on the current action from the q-value without changing the (maximum of our) objecttive function $J$:  \n",
    "\n",
    "$$ \\nabla J(\\boldsymbol{\\theta}) \\propto \\sum_{s} \\mu(s) \\sum_a \\left( q_{\\pi}(s, a) - b(s) \\right) \\nabla \\pi(a \\mid s, \\boldsymbol{\\theta}) $$\n",
    "\n",
    "In `REINFORCE` this means we can subtract from our target $G_t$:\n",
    "\n",
    "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha (G_t - b(S_t)) \\frac{\\nabla \\pi(A_t \\mid s, \\boldsymbol{\\theta})}{\\pi(A_t \\mid s, \\boldsymbol{\\theta})} $$\n",
    "\n",
    "Since we are only interested in the **maximum** of our objective, we can also **rescale** our target by any function that also doesn't depend on the action. A **simple baseline** which is even independent of the state -- that is, it is **constant** for each episode -- is to just **standardize rewards within the episode**. So, we **subtract** the average return and **divide** by the variance of returns:\n",
    "\n",
    "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha \\left(\\frac{G_t - \\bar{G}}{\\sigma_G}\\right) \\nabla  \\pi(A_t \\mid s, \\boldsymbol{\\theta}) $$\n",
    "\n",
    "This baseline is **already** implemented in my implementation of `REINFORCE`. Experiment with and without this standardization baseline and compare the performance. We are going to do something more interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf5b245-9369-4ed9-b5f4-3a52bb975d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Modify your implementation of `REINFORCE` to optionally use the standardize baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cfde8e-f1c7-4d96-85e0-268f687e09df",
   "metadata": {},
   "source": [
    "**The Real Exercise**: Standard practice is to use the state-value function $v(s)$ as a baseline. This is intuitively appealing -- we are more interested in updating out policy for returns that estimate the current **value** worse. Our new update becomes:\n",
    "\n",
    "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha (G_t - \\tilde{v}(S_t \\mid \\mathbf{w})) \\frac{\\nabla \\pi(A_t \\mid s, \\boldsymbol{\\theta})}{\\pi(A_t \\mid s, \\boldsymbol{\\theta})} $$\n",
    "\n",
    "where $\\tilde{v}(s \\mid \\mathbf{w})$ is a **deep neural network** with parameters $w$ that estimates $v_\\pi(s)$. What neural network? Typically, we use the **same** network architecture as that of the Policy.\n",
    "\n",
    "**Your Task**: Modify your implementation to fit a second, baseline network to estimate the value function and use it as **baseline**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4de3a2a8-05c3-44e2-9858-8cc8f431325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf1447-d222-4b24-a357-5b7f9824390c",
   "metadata": {},
   "source": [
    "-----\n",
    "## Exercise 3: Going Deeper\n",
    "\n",
    "As usual, pick **AT LEAST ONE** of the following exercises to complete.\n",
    "\n",
    "### Exercise 3.1: Solving Lunar Lander with `REINFORCE` (easy)\n",
    "\n",
    "Use my (or even better, improve on my) implementation of `REINFORCE` to solve the [Lunar Lander Environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/). This environment is a little bit harder than Cartpole, but not much. Make sure you perform the same types of analyses we did during the lab session to quantify and qualify the performance of your agents.\n",
    "\n",
    "### Exercise 3.2: Solving Cartpole and Lunar Lander with `Deep Q-Learning` (harder)\n",
    "\n",
    "On policy Deep Reinforcement Learning tends to be **very unstable**. Write an implementation (or adapt an existing one) of `Deep Q-Learning` to solve our two environments (Cartpole and Lunar Lander). To do this you will need to implement a **Replay Buffer** and use a second, slow-moving **target Q-Network** to stabilize learning.\n",
    "\n",
    "### Exercise 3.3: Solving the OpenAI CarRacing environment (hardest) \n",
    "\n",
    "Use `Deep Q-Learning` -- or even better, an off-the-shelf implementation of **Proximal Policy Optimization (PPO)** -- to train an agent to solve the [OpenAI CarRacing](https://github.com/andywu0913/OpenAI-GYM-CarRacing-DQN) environment. This will be the most *fun*, but also the most *difficult*. Some tips:\n",
    "\n",
    "1. Make sure you use the `continuous=False` argument to the environment constructor. This ensures that the action space is **discrete** (we haven't seen how to work with continuous action spaces).\n",
    "2. Your Q-Network will need to be a CNN. A simple one should do, with two convolutional + maxpool layers, folowed by a two dense layers. You will **definitely** want to use a GPU to train your agents.\n",
    "3. The observation space of the environment is a single **color image** (a single frame of the game). Most implementations stack multiple frames (e.g. 3) after converting them to grayscale images as an observation.\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
